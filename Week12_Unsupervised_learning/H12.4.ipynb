{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA for dimensionality reduction before classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the `data.csv` file to this workspace, then read in the data to a `numpy` array in `X` and the labels to `y`. If you want to, you can add code to the following cell to explore `X` (for example, see its shape).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.genfromtxt('data.csv',delimiter=',')\n",
    "X = dat[:, :-1]\n",
    "y = dat[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `train_test_split` to split the data into training and test sets. Reserve 30% of the data for the test set. \n",
    "\n",
    "Make sure to shuffle the data, and pass `random_state = 42` so that your random split will match the auto-grader's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the training data to fit a support vector classifier. However, instead of fitting the training data directly, you will first transform it using PCA. Then, you will use only a subset of features - the first `n_comp` principal components - as input to your classifier. \n",
    "\n",
    "You will use K-fold cross validation to find the optimal value of `n_comp`. You should consider every possible value of `n_comp`, from 1 component (simplest possible model) to all of the components (most flexible model).\n",
    "\n",
    "In the next cell,\n",
    "\n",
    "* Use the `sklearn` implementation of `KFold` to iterate over candidate models. In your `KFold`, use 5 splits, and don't shuffle the data (you already shuffled it when dividing into training and test.)\n",
    "* Use the `sklearn` implementation of `PCA` to transform the data. Pass `random_state = 42` to `PCA` so that your result will match the auto-grader's.\n",
    "* Use the `sklearn` implementation of `SVC` to classify the data using the first `n_comp` principal components.  Pass `random_state = 42` to `SVC` so that your result will match the auto-grader's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "average_accuracy = np.zeros(X_train.shape[1])\n",
    "average_validation = np.zeros(X_train.shape[1])\n",
    "pca = PCA(n_components=X_train.shape[1], random_state=42)\n",
    "for n_comp in range(1,X_train.shape[1]+1):\n",
    "    # pca = PCA(n_components=n_comp, random_state=42)\n",
    "    fold_acc = np.zeros(kf.get_n_splits())\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "        pca.fit(X_train_fold)\n",
    "        X_train_fold_pca = pca.transform(X_train_fold)[:,:n_comp]\n",
    "        X_test_fold_pca = pca.transform(X_test_fold)[:,:n_comp]\n",
    "        svc = SVC(random_state=42)\n",
    "        svc.fit(X_train_fold_pca, y_train_fold)\n",
    "        y_pred = svc.predict(X_test_fold_pca)\n",
    "        fold_acc[i] = accuracy_score(y_test_fold, y_pred)\n",
    "    average_accuracy[n_comp-1] = fold_acc.mean()\n",
    "    average_validation[n_comp-1] = fold_acc.std()/np.sqrt(kf.get_n_splits()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0451754 , 0.05248907, 0.03642157, 0.04164966, 0.06624013,\n",
       "       0.05050763, 0.04164966, 0.04164966, 0.04164966, 0.05714286,\n",
       "       0.04738035, 0.04738035, 0.04738035, 0.04738035, 0.04738035,\n",
       "       0.04738035, 0.04738035, 0.04738035, 0.04738035, 0.04738035])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64285714, 0.62857143, 0.67142857, 0.65714286, 0.68571429,\n",
       "       0.71428571, 0.72857143, 0.72857143, 0.72857143, 0.7       ,\n",
       "       0.7       , 0.7       , 0.7       , 0.7       , 0.7       ,\n",
       "       0.7       , 0.7       , 0.7       , 0.7       , 0.7       ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean validation accuracy and the standard error of the mean validation accuracy across the folds. Save the results in `acc_mean` and `acc_se`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "\n",
    "acc_mean = average_accuracy\n",
    "acc_se = average_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, compute the optimal value of `n_comp`, and save this in `n_pca_opt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "\n",
    "n_pca_opt = np.argmax(average_accuracy)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compute the optimal `n_comp` according to the one-SE rule, and save this in `n_pca_one_se`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "\n",
    "max_avg_accuracy = np.max(average_accuracy)\n",
    "max_se = average_validation[np.argmax(average_accuracy)]\n",
    "\n",
    "# Calculate the threshold for the one-SE rule\n",
    "threshold = max_avg_accuracy - max_se\n",
    "\n",
    "# Apply the one-SE rule to find the simplest model within one standard error of the best model\n",
    "n_pca_one_se = np.where(average_accuracy >= threshold)[0][0] + 1  # Adding 1 because array indices start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pca_one_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
